{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 1: Analysis of Tokenization and N-grams in Natural Language Processing\\"
      ],
      "metadata": {
        "id": "BXh97Jh2Ms6-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "6bhFHlQwLQXb",
        "outputId": "c42da52d-753e-4324-90f9-a8a055180903"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Sample document\n",
        "document = \"Natural language processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and humans using natural language.\"\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(document.lower())\n",
        "\n",
        "# Total number of tokens\n",
        "total_tokens = len(tokens)\n",
        "\n",
        "# Number of unique tokens\n",
        "unique_tokens = set(tokens)\n",
        "num_unique_tokens = len(unique_tokens)\n",
        "\n",
        "# Token frequency distribution\n",
        "token_freq = FreqDist(tokens)\n",
        "\n",
        "# Generate N-grams\n",
        "n = 2  # Change to desired n-gram size\n",
        "n_grams = list(ngrams(tokens, n))\n",
        "\n",
        "# N-gram frequency distribution\n",
        "n_gram_freq = FreqDist(n_grams)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total number of tokens:\", total_tokens)\n",
        "print(\"Number of unique tokens:\", num_unique_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jphyS8LYO-Nf",
        "outputId": "b0b410f2-55be-4a6a-d124-b1da9b205716"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of tokens: 28\n",
            "Number of unique tokens: 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nToken frequency distribution:\")\n",
        "for token, freq in token_freq.items():\n",
        "    print(f\"{token}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "m0uXWlTXO_6W",
        "outputId": "6435d4ad-6399-4e09-a1ee-46f770e7b732"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Token frequency distribution:\n",
            "natural: 2\n",
            "language: 2\n",
            "processing: 1\n",
            "(: 2\n",
            "nlp: 1\n",
            "): 2\n",
            "is: 1\n",
            "a: 1\n",
            "subfield: 1\n",
            "of: 1\n",
            "artificial: 1\n",
            "intelligence: 1\n",
            "ai: 1\n",
            "that: 1\n",
            "focuses: 1\n",
            "on: 1\n",
            "the: 1\n",
            "interaction: 1\n",
            "between: 1\n",
            "computers: 1\n",
            "and: 1\n",
            "humans: 1\n",
            "using: 1\n",
            ".: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTop 5 Tokens:\")\n",
        "for token, freq in token_freq.most_common(5):\n",
        "    print(f\"{token}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "e93voDl4PBdf",
        "outputId": "0e9768d2-07f3-4b1a-ced2-7fcb1c060ced"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 5 Tokens:\n",
            "natural: 2\n",
            "language: 2\n",
            "(: 2\n",
            "): 2\n",
            "processing: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nN-gram frequency distribution:\")\n",
        "for n_gram, freq in n_gram_freq.items():\n",
        "    print(f\"{' '.join(n_gram)}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-RgmXWOwPCjm",
        "outputId": "caa2b7a6-bd8d-4e75-c5ed-6f18af02fb74"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "N-gram frequency distribution:\n",
            "natural language: 2\n",
            "language processing: 1\n",
            "processing (: 1\n",
            "( nlp: 1\n",
            "nlp ): 1\n",
            ") is: 1\n",
            "is a: 1\n",
            "a subfield: 1\n",
            "subfield of: 1\n",
            "of artificial: 1\n",
            "artificial intelligence: 1\n",
            "intelligence (: 1\n",
            "( ai: 1\n",
            "ai ): 1\n",
            ") that: 1\n",
            "that focuses: 1\n",
            "focuses on: 1\n",
            "on the: 1\n",
            "the interaction: 1\n",
            "interaction between: 1\n",
            "between computers: 1\n",
            "computers and: 1\n",
            "and humans: 1\n",
            "humans using: 1\n",
            "using natural: 1\n",
            "language .: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTop 5 Bi-grams:\")\n",
        "for n_gram, freq in n_gram_freq.most_common(5):\n",
        "    print(f\"{' '.join(n_gram)}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "l2PlV05kPDsX",
        "outputId": "0bb56694-572e-4bc8-9f08-75c7278352c9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 5 Bi-grams:\n",
            "natural language: 2\n",
            "language processing: 1\n",
            "processing (: 1\n",
            "( nlp: 1\n",
            "nlp ): 1\n"
          ]
        }
      ]
    }
  ]
}