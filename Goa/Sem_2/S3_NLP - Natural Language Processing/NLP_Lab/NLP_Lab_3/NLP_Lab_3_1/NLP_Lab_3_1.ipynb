{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lm3bjWXX6Isp"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "class NGramLanguageModel:\n",
        "    def __init__(self, n):\n",
        "        \"\"\"\n",
        "        Initialize the N-gram Language Model\n",
        "\n",
        "        :param n: Order of the N-gram model (2 for bigram, 3 for trigram, etc.)\n",
        "        \"\"\"\n",
        "        self.n = n\n",
        "        self.vocabulary = set()\n",
        "        self.unigram_counts = Counter()\n",
        "        self.ngram_counts = defaultdict(Counter)\n",
        "        self.total_tokens = 0\n",
        "        self.smoothing_alpha = 0.1\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Preprocess the input text\n",
        "\n",
        "        :param text: Input text string\n",
        "        :return: List of tokenized and cleaned words\n",
        "        \"\"\"\n",
        "        # Convert to lowercase and split into words\n",
        "        words = re.findall(r'\\w+', text.lower())\n",
        "\n",
        "        # Add start and end tokens\n",
        "        padded_words = ['<s>'] * (self.n - 1) + words + ['</s>']\n",
        "\n",
        "        return padded_words\n",
        "\n",
        "    def train_unsmoothed(self, corpus):\n",
        "        \"\"\"\n",
        "        Train the unsmoothed N-gram model\n",
        "\n",
        "        :param corpus: List of text documents\n",
        "        \"\"\"\n",
        "        # Reset model parameters\n",
        "        self.unigram_counts = Counter()\n",
        "        self.ngram_counts = defaultdict(Counter)\n",
        "        self.vocabulary = set()\n",
        "        self.total_tokens = 0\n",
        "\n",
        "        # Process each document in the corpus\n",
        "        for doc in corpus:\n",
        "            tokens = self.preprocess_text(doc)\n",
        "\n",
        "            # Update unigram counts\n",
        "            self.unigram_counts.update(tokens)\n",
        "            self.vocabulary.update(tokens)\n",
        "            self.total_tokens += len(tokens)\n",
        "\n",
        "            # Update N-gram counts\n",
        "            for i in range(len(tokens) - self.n + 1):\n",
        "                context = tuple(tokens[i:i+self.n-1])\n",
        "                current_word = tokens[i+self.n-1]\n",
        "                self.ngram_counts[context][current_word] += 1\n",
        "\n",
        "    def train_smoothed(self, corpus, alpha=0.1):\n",
        "        \"\"\"\n",
        "        Train the smoothed (Laplace/Add-alpha) N-gram model\n",
        "\n",
        "        :param corpus: List of text documents\n",
        "        :param alpha: Smoothing parameter\n",
        "        \"\"\"\n",
        "        # Reset model parameters\n",
        "        self.unigram_counts = Counter()\n",
        "        self.ngram_counts = defaultdict(Counter)\n",
        "        self.vocabulary = set()\n",
        "        self.total_tokens = 0\n",
        "        self.smoothing_alpha = alpha\n",
        "\n",
        "        # Process each document in the corpus\n",
        "        for doc in corpus:\n",
        "            tokens = self.preprocess_text(doc)\n",
        "\n",
        "            # Update unigram counts\n",
        "            self.unigram_counts.update(tokens)\n",
        "            self.vocabulary.update(tokens)\n",
        "            self.total_tokens += len(tokens)\n",
        "\n",
        "            # Update N-gram counts\n",
        "            for i in range(len(tokens) - self.n + 1):\n",
        "                context = tuple(tokens[i:i+self.n-1])\n",
        "                current_word = tokens[i+self.n-1]\n",
        "                self.ngram_counts[context][current_word] += 1\n",
        "\n",
        "    def calculate_probability_unsmoothed(self, context, word):\n",
        "        \"\"\"\n",
        "        Calculate probability for unsmoothed model\n",
        "\n",
        "        :param context: Preceding N-1 words\n",
        "        :param word: Current word\n",
        "        :return: Probability of the word given the context\n",
        "        \"\"\"\n",
        "        context_count = sum(self.ngram_counts[context].values())\n",
        "        if context_count == 0:\n",
        "            return 0.0\n",
        "\n",
        "        return self.ngram_counts[context][word] / context_count\n",
        "\n",
        "    def calculate_probability_smoothed(self, context, word):\n",
        "        \"\"\"\n",
        "        Calculate probability for smoothed (Laplace) model\n",
        "\n",
        "        :param context: Preceding N-1 words\n",
        "        :param word: Current word\n",
        "        :return: Smoothed probability of the word given the context\n",
        "        \"\"\"\n",
        "        # Number of unique words in vocabulary\n",
        "        vocab_size = len(self.vocabulary)\n",
        "\n",
        "        # Count of current n-gram\n",
        "        ngram_count = self.ngram_counts[context][word]\n",
        "\n",
        "        # Count of context\n",
        "        context_count = sum(self.ngram_counts[context].values())\n",
        "\n",
        "        # Laplace smoothing\n",
        "        smoothed_prob = (ngram_count + self.smoothing_alpha) / \\\n",
        "                        (context_count + self.smoothing_alpha * vocab_size)\n",
        "\n",
        "        return smoothed_prob\n",
        "\n",
        "    def calculate_perplexity(self, test_corpus, smoothed=False):\n",
        "        \"\"\"\n",
        "        Calculate perplexity of the model on test corpus\n",
        "\n",
        "        :param test_corpus: List of text documents for testing\n",
        "        :param smoothed: Whether to use smoothed probabilities\n",
        "        :return: Perplexity score\n",
        "        \"\"\"\n",
        "        log_prob_sum = 0\n",
        "        total_test_tokens = 0\n",
        "\n",
        "        for doc in test_corpus:\n",
        "            tokens = self.preprocess_text(doc)\n",
        "\n",
        "            for i in range(len(tokens) - self.n + 1):\n",
        "                context = tuple(tokens[i:i+self.n-1])\n",
        "                current_word = tokens[i+self.n-1]\n",
        "\n",
        "                # Select probability calculation method\n",
        "                if smoothed:\n",
        "                    prob = self.calculate_probability_smoothed(context, current_word)\n",
        "                else:\n",
        "                    prob = self.calculate_probability_unsmoothed(context, current_word)\n",
        "\n",
        "                # Avoid log(0)\n",
        "                prob = max(prob, 1e-10)\n",
        "\n",
        "                log_prob_sum += math.log(prob)\n",
        "                total_test_tokens += 1\n",
        "\n",
        "        # Calculate perplexity\n",
        "        avg_log_prob = log_prob_sum / total_test_tokens\n",
        "        perplexity = math.exp(-avg_log_prob)\n",
        "\n",
        "        return perplexity\n",
        "\n",
        "    def print_model_information(self):\n",
        "        \"\"\"\n",
        "        Print comprehensive information about the trained N-gram model\n",
        "        \"\"\"\n",
        "        print(\"\\n--- N-gram Language Model Information ---\")\n",
        "        print(f\"N-gram Order: {self.n}\")\n",
        "        print(f\"Total Tokens: {self.total_tokens}\")\n",
        "        print(f\"Vocabulary Size: {len(self.vocabulary)}\")\n",
        "\n",
        "        # Vocabulary Details\n",
        "        print(\"\\n--- Vocabulary ---\")\n",
        "        print(\"Top 10 Most Frequent Words:\")\n",
        "        for word, count in self.unigram_counts.most_common(10):\n",
        "            print(f\"{word}: {count}\")\n",
        "\n",
        "        # N-gram Details\n",
        "        print(f\"\\n--- {self.n}-gram Statistics ---\")\n",
        "        print(\"Number of Unique Contexts:\", len(self.ngram_counts))\n",
        "\n",
        "        # Top N-grams\n",
        "        print(\"\\nTop 10 Most Frequent N-grams:\")\n",
        "        top_ngrams = []\n",
        "        for context, word_counts in list(self.ngram_counts.items())[:10]:\n",
        "            for word, count in word_counts.most_common(1):\n",
        "                top_ngrams.append((context, word, count))\n",
        "\n",
        "        for context, word, count in sorted(top_ngrams, key=lambda x: x[2], reverse=True):\n",
        "            print(f\"Context {context} -> Word '{word}': {count} times\")\n",
        "\n",
        "        # Probability Distribution\n",
        "        print(\"\\n--- Probability Distribution ---\")\n",
        "        print(\"Smoothing Alpha:\", self.smoothing_alpha)\n",
        "\n",
        "        # Sample Probability Calculations\n",
        "        print(\"\\nSample Probability Calculations:\")\n",
        "        contexts_to_sample = list(self.ngram_counts.keys())[:5]\n",
        "        for context in contexts_to_sample:\n",
        "            print(f\"\\nContext: {context}\")\n",
        "            # Get top 3 most probable words for this context\n",
        "            top_words = sorted(\n",
        "                [(word, self.calculate_probability_smoothed(context, word))\n",
        "                 for word in self.ngram_counts[context].keys()],\n",
        "                key=lambda x: x[1],\n",
        "                reverse=True\n",
        "            )[:3]\n",
        "\n",
        "            for word, prob in top_words:\n",
        "                print(f\"  {word}: {prob:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample corpus\n",
        "train_corpus = [\n",
        "    \"the quick brown fox jumps over the lazy dog\",\n",
        "    \"a quick brown dog jumps over the lazy fox\",\n",
        "    \"the lazy fox sleeps all day\"\n",
        "]\n",
        "\n",
        "test_corpus = [\n",
        "    \"quick brown animal jumps\",\n",
        "    \"lazy fox sleeps\"\n",
        "]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RVnhYMBk7OPA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bigram model\n",
        "bigram_model = NGramLanguageModel(n=2)"
      ],
      "metadata": {
        "id": "KJtRniW68UUf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train unsmoothed model\n",
        "bigram_model.train_unsmoothed(train_corpus)\n",
        "unsmoothed_perplexity = bigram_model.calculate_perplexity(test_corpus, smoothed=False)\n",
        "print(f\"Unsmoothed Bigram Perplexity: {unsmoothed_perplexity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__YoHBVa8TF3",
        "outputId": "71976cb8-4db0-4127-a1a4-cbace7e2c75e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsmoothed Bigram Perplexity: 5485874.080739646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print detailed model information\n",
        "bigram_model.print_model_information()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4htA7lfc8RQA",
        "outputId": "8a75b9a2-b2e6-4da7-de28-cb616724251a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- N-gram Language Model Information ---\n",
            "N-gram Order: 2\n",
            "Total Tokens: 30\n",
            "Vocabulary Size: 14\n",
            "\n",
            "--- Vocabulary ---\n",
            "Top 10 Most Frequent Words:\n",
            "the: 4\n",
            "<s>: 3\n",
            "fox: 3\n",
            "lazy: 3\n",
            "</s>: 3\n",
            "quick: 2\n",
            "brown: 2\n",
            "jumps: 2\n",
            "over: 2\n",
            "dog: 2\n",
            "\n",
            "--- 2-gram Statistics ---\n",
            "Number of Unique Contexts: 14\n",
            "\n",
            "Top 10 Most Frequent N-grams:\n",
            "Context ('the',) -> Word 'lazy': 3 times\n",
            "Context ('<s>',) -> Word 'the': 2 times\n",
            "Context ('quick',) -> Word 'brown': 2 times\n",
            "Context ('jumps',) -> Word 'over': 2 times\n",
            "Context ('over',) -> Word 'the': 2 times\n",
            "Context ('lazy',) -> Word 'fox': 2 times\n",
            "Context ('brown',) -> Word 'fox': 1 times\n",
            "Context ('fox',) -> Word 'jumps': 1 times\n",
            "Context ('dog',) -> Word '</s>': 1 times\n",
            "Context ('a',) -> Word 'quick': 1 times\n",
            "\n",
            "--- Probability Distribution ---\n",
            "Smoothing Alpha: 0.1\n",
            "\n",
            "Sample Probability Calculations:\n",
            "\n",
            "Context: ('<s>',)\n",
            "  the: 0.4773\n",
            "  a: 0.2500\n",
            "\n",
            "Context: ('the',)\n",
            "  lazy: 0.5741\n",
            "  quick: 0.2037\n",
            "\n",
            "Context: ('quick',)\n",
            "  brown: 0.6176\n",
            "\n",
            "Context: ('brown',)\n",
            "  fox: 0.3235\n",
            "  dog: 0.3235\n",
            "\n",
            "Context: ('fox',)\n",
            "  jumps: 0.2500\n",
            "  </s>: 0.2500\n",
            "  sleeps: 0.2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train smoothed model\n",
        "bigram_model.train_smoothed(train_corpus, alpha=0.1)\n",
        "smoothed_perplexity = bigram_model.calculate_perplexity(test_corpus, smoothed=True)\n",
        "print(f\"\\nSmoothed Bigram Perplexity: {smoothed_perplexity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8Pr-G-b8QfP",
        "outputId": "e014d4c8-40fb-4169-8393-c5c6cd9d4ae1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Smoothed Bigram Perplexity: 12.94446006439085\n"
          ]
        }
      ]
    }
  ]
}